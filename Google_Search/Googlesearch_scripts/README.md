Final Proposal
Johnny_Data606 Final proposal

## What is your issue of interest (provide sufficient background information)?
Stock price is entirely governed by 2 factors: What the owner wants to sell it for, and what the buyer is willing to pay for it.  Many factors influence their decisions.  This project will focus on price prediction based on semantic analysis and sentiment analysis of social media, google search terms, and new articles over time.  
## Why is this issue important to you and/or to others?
Stock market price prediction can be lucrative if it can be done effectively and reliably.  $90B of stock is traded on a daily basis in the U.S.  Establishing machine learning methods to trade stock, establish stop loss predictions, and getting in on market movements can be important commercial opportunities that a skilled data scientist can support.   
## What questions do you have in mind and would like to answer?
I’m very interested in the semantics of stock advice expressed by authors and people in their social media posts.  What is the intent of any particular post?  Sentiment is easy to quantify, but what is the sentiment about?  What was the intent of the post?  Was it informational, directive, or influencing?  What is the spectrum of intent expressed in these corpora? How successful would predictions be if intent, and semantic modeling were included along with sentiment?
## How do sellers and buyers make decisions to buy or sell, and how can their behaviors and external influencers be examined, inferred, forecasted, and modeled?  
Investor behavior can be witnessed as communication occurs.  Capture of social media posts for semantic modeling and sentiment can provide indicators of public sentiment and explain market pressures at play in price setting in the market.  Modeling the relationship of media publications semantics, intent, and sentiment to stock prices as “influencers” can be analyzed to determine the level of influence on investor decision making.  As a predictive data source these can be evaluated using neural networks, scoring, including, and/or excluding data elements to improve the effectiveness of price predictions.  If there are influencers at play that are guiding investor behaviors, machine learning could be leveraged to gather and filter the available data sources to produce reliable stock price predictions.  
## Where do you get the data to analyze and help answer your questions (creditability of source, quality of data, size of data, attributes of data. etc.)?
Stock price data is available from many sources.  Yahoo Finance provides an api where historical stock data is available.  Social media platforms provide access to social media posts.  Twitter and Reddit are lucrative in the scale and scope of available data sets. Google provides a wealth of search opportunities to find news sources.  Top level domains can be explored to discover the availability of news article sources.  
## What will be your unit of analysis (for example, patient, organization, or country)? Roughly how many units (observations) do you expect to analyze?
The unit of analysis will be stock price predictions of several high-volume stocks.  Some preliminary stocks will be stocks that have risen in value over the course of the last 10 years.  AAPL, TSLA, AMZN, ALPHABET, etc.  
## What variables/measures do you plan to use in your analysis (variables should be tied to the questions in #3)?
Semantic weights, topic modeling, sentiment analysis.  Intent modeling of stock influencers to determine the polarity and magnitude of influence being expressed.  Is the influencer bullish or bearish?  In sentiment analysis there is a concept of polarity.  Positive or negative.  Magnitude, subjectivity, and sarcasm can be modeled.  How can semantic intent of bullish and bearish influence be modeled on publications to create a bull/bear polarity and magnitude score?
## What kinds of techniques/models do you plan to use (for example, clustering, NLP, ARIMA, etc.)?
Semantic modeling using word vector distance calculations among influence terms.  Clustering of topics and articles, cosine distance calculations of document similarity and sentiment scores can be established to establish consensus or disorder in publications to infer strength or weakness of influence being exerted.  This can be evaluated in contrast to consumer confident indexes. Various predictive models associated with time series data sets will be used to determine which models are successful.  
## How do you plan to develop/apply ML and how you evaluate/compare the performance of the models?
Model performance will be evaluated using independent models compared using their price predictions against actual prices.  Having several predictive models run against the same corpus sources can evaluate the predictive power of the model as well as the predictive nature of the source corpus.  Electing winners or combining, filtering, or aggregating predictive models could be used to leverage strengths and mitigate weaknesses in predictive models to produce higher performance4. 
## What outcomes do you intend to achieve (better understanding of problems, tools to help solve problems, predictive analytics with practical applications, etc)?
It is perhaps naive to assume that models created in this project will net a stable and reliable stock prediction methodology that does not require actual domain knowledge of the forces that drive stock prices.  However, the methodology of evaluating investor behaviors and influencers is useful in many domains. Focusing on news media and social media can provide access to expert and pseudo-expert advice which presumably could be used to predict stock price. Thus, the focus shifts to modeling the predictive utility of the author and the advice to predicted price. Machine learning models produced would reflect a predictive score of the specific author and a semantic score of the advice.   

# Stock Price Governed by 2 factors:  
## What the seller wants to sell it for
#What the buyer is willing to pay for it
This project will be based on developing a baseline of social media and news article sources from which to extract semantics and sentiments expressed regarding stock valuation.  As the investor market behaves by buying and selling, that behavior is expressed as stock transactions.  These transactions are well known in terms of which stock is bought, what the volume of the sale was, and what the price was.   The stock in question has characteristics such as publications to the stock exchange detailing the company’s business, insider trading, dividends, etc.  
# In the age of the internet:
## News and Social Media have immediate impact to investors in the age of the internet
In the age of the internet and social media, investors are able to consume a wide variety of news and social media sources and can express their knowledge and opinions in social media, publications, blogs, etc.  From a data science perspective these sources can be mined to identify what the authors know, think, and feel about the stock, the company, the leadership of the company, the products the company produces, the value of the stock, and advice on investing.  In stock market terms these expressions may express a bull market or a bear market semantic and sentiment.  In other words, knowledge or belief that the stock will go up in price or down at any given time?  
# Project data sources and methods
The project will collect up to 1000 articles per day and 1000 social media sources per day on the target stocks.  The articles will be processed using newspaper3k python library and beautiful soup to extract metadata on the article, top domain, publication, author, date, topics, etc.  The content will be tokenized using spacy to perform tokenization and produce word vectors.  One main objective of this process is to establish an analysis process to identify the semantics of bear and bull market expressions among the authors of the corpus.  Beyond simple positivity, negativity, subjectivity, and polarity of sentiment analysis, what advice is being expressed by the author?  Using word vectors, can a model be developed that extracts the semantics of what the author is advising?  In keeping with the concepts of sentiment analysis where a corpus is evaluated for polarity and magnitude, the theory driving this project is that the corpora can be examined for a spectrum of semantic advice where the semantics are expressed in terms of a bear to bull score.  Where bull would be a positive 1 and a bear a negative 1.  
# Source attribution
Each news article and social media tweet will be attributed to its source and author.  These sources will be processed for sentiment and semantics and a score provided.  The data sets will be time series data and will be treated as discrete features for the purpose of machine learning.  Techniques such as summarization, TF-IDF, topic modeling, LDA, etc will be leveraged to reduce the data sets to a manageable corpora.
# 2 main objectives for Machine learning LSTM
Long and Short Term Memory (LSTM) recurrent neural network will be leveraged to examine the sentiment scores and word vectors of these sources to predict stock price.  There are 2 main objectives for the machine learning component.  
# First: Predictive accuracy of source
First is to develop a predictive accuracy score of the author and advice as it relates to predicting stock price.  The theory being that there may be some expertise expressed in publications that are reliable predictors of stock price.  If this expertise can be examined and modeled in a predictive accuracy score, it may be possible to follow these authors advise and drive machine trading.  The problem is that even if there are reliable pundits that are providing useful and valuable advice, how can their advice be identified as valuable and filtered out of the noise of other authors expressing faulty or misguiding advice?  This is where machine learning could come into play.  
# Author as a feature
Treating the individual author as a feature in a neural network predictor such that all authors are considered over time as a discrete input, then as the neural network learns how to predict the stock price based on their content, certain authors will be more useful in those predictions than others.  The hope is that the individual authors can be scored such that authors expressing advice that accurately predicts price fluctuations will be included and weighted higher or weighted lower or excluded if their advice is inaccurate.  As this model is developed the hope is that the model will provide a predictive accuracy score such that expert advise can be culled from noisy media. 
# Second: Semantic Bear/Bull Model
The second is to develop the semantic model itself identifying the specific word vectors in the successful prediction corpora which are indicators that drive either buy or sell predictions.  As the price prediction model is developed the neural network will have identified the inputs and weighting associated with the prediction.  These inputs and weights will then inform the process of extracting the word vectors in the corpora that most influence the model.  As the word vector space is examined the hope is that a model can be developed that would provide a bull/bear score of a corpus such that a simple score can be calculated that will be similar to a sentiment score but applied as a semantic score directly attributing the author’s semantic bull/bear intent.  
# Data Collection
Data collection is problematic from a freely available data source perspective.  I.E. free data is not easy to obtain and the scale and availability of data is constrained by several factors.  Search engine APIs such as Google are severely constrained at the free price point and social media APIs are constrained with respect to historical access.  There are work arounds for both. However, the complexity of data collection and mitigation methodology will be shared as part of the project.  For example, Google API constrains the query results to a limit of 100.  Direct web scraping of search engines and web sites is interpreted as a robot and access is subsequently sanctioned by the platform. Therefore, various methodologies such as wait times embedded in queries and VPN proxy servers embedded in web scraping activities will be employed.  Social media access is also constrained, however as a research student Twitter has provided me with a research associate account that provides retrospective historical search capabilities.  If the modeling developed in this project were to become commercially viable, then both search engine and social media accounts could resolve these difficulties through paid access to the required data.  
# Data Access
In terms of data access, this project will mine Reddit, and Twitter for social media corpora, Google and Yahoo for search engine data to locate urls of news articles and blogs, and mine the resulting corpora to extract text and metadata identifying the author, platform, and top level domain.  Stock price data will be obtained from Yahoo finance and Alphavantage.  
# Compute resources
Compute resources will include Google Colab, personal laptop pc, and a dell R810 server that I have built in my home office.  Currently GPU resources are available in the colab and pc environments.  As the project progresses the requirement for GPU resources will become clearer.  The specific resources used in the project will be explained as the project proceeds.  
# Potential project scaling issue
It is easy at this stage in the project to theorize on a higher scale than is possible with the time, data, and compute resource constraints.  As these constraints are discovered in the project, they will be shared along with recommendations regarding how they could be resolved in a commercial context if this project were to be developed in a “for profit” context.  

# week 
google_search_returns_urls jupyter Notebook provides urls from google that are based on a time based search of a search term.  

These URLs are placed into a dataframe and stored along side the search term and date.  
The date is a one day span of time for each loop.  The initial run is intended to be for a single term.  AAPL.  Subsequent runs will be for adjacent search terms intended to pull in information about the same stock.  

This is a first step.  the loop in the script will query google but the problem is runing this script will make google mad at you.  Before we run this script we need a vpn script that runs the script through a random vpn to keep google from getting mad at you. 

the basic problem I'm working through is the issue of learning git and python scripting.  

I found that nteracting with the git GUI and the Ggit command line creates a problem between the two.  

If I hit a button on the gui then when I run commands on the cvommand line, there is a differential in the --staged files that make problems for me.  Learning curves are a problem!
